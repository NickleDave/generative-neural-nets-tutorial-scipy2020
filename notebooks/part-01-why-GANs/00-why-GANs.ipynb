{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generative model:** For the purposes of this tutorial, \"any model that takes a training set, consisting of samples drawn from a distribution $p_{data}$, and learns to represent an estimate of that distribution somehow.\" [Goodfellow 2016]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo: first-order Markov model\n",
    "\n",
    "A simple model many might be familiar with that can be seen as generative is a **first-order Markov model**, \n",
    "which models transitions between states as a matrix.\n",
    "\n",
    "Say that our data a vector fo values indicating whether it rained or not for the last 25 days (where a value of `1` means it rained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rained = np.random.randint(2, size=25)\n",
    "print(rained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we \"fit\" our Markov model?\n",
    "\n",
    "What we want is a matrix where each element represents the probability of transition from state $i$ to state $j$.\n",
    "\n",
    "To compute this we count the occurrence of each possible transition `{0 -> 0, 0 ->1, 1 -> 0, 1 -> 1}` and then normalize the counts in each row by the sum of counts in that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_markov(data):\n",
    "    trans_mat = np.zeros((2, 2))\n",
    "\n",
    "    for ind, transition in enumerate(rained):\n",
    "        if ind == 0:\n",
    "            prev_state = transition\n",
    "        else:\n",
    "            trans_mat[prev_state][transition] += 1\n",
    "            prev_state = transition\n",
    "\n",
    "    trans_mat /= trans_mat.sum(1)[:, np.newaxis]\n",
    "    \n",
    "    return trans_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61538462 0.38461538]\n",
      " [0.45454545 0.54545455]]\n"
     ]
    }
   ],
   "source": [
    "trans_mat = fit_markov(rained)\n",
    "print(trans_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can *generate* new data from the distribution we have fit.\n",
    "\n",
    "A simple way to do so is, for each time step:\n",
    "- take the row of our matrix corresponding to our current state\n",
    "- sampling from the uniform distribution\n",
    "- use that sample to decide which state we are in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n_steps, trans_mat):\n",
    "    current_state = np.random.randint(2, size=1).item()  # initial state, assume 50 / 50 chance of rain or no for example\n",
    "    fake = [current_state]\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        trans_mat_this_state = np.cumsum(trans_mat[current_state])\n",
    "        current_state = np.where(\n",
    "             trans_mat_this_state > np.random.uniform()\n",
    "        )[0][0]\n",
    "        fake.append(current_state)\n",
    "\n",
    "    fake = np.asarray(fake)\n",
    "    return fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "n_steps = 25\n",
    "\n",
    "fake_rained = generate(n_steps, trans_mat)\n",
    "\n",
    "print(fake_rained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a taxonomy of generative models\n",
    "explicit v. implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why do we want generative models in our research?\n",
    "\n",
    "1. test \"our ability to represent and manipulate high-dimensional probability distributions\" [Goodfellow 2016]\n",
    "\n",
    "### demo: GalaxyGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. \"many tasks intrinsically require realistic generation of samples from some distribution.\" [Goodfellow 2016]\n",
    "\n",
    "### demo: single-image super resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. fit models that would otherwise be intractable or computationally expensive\n",
    "  + e.g., require variational and/or Monte Carlo approximation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
