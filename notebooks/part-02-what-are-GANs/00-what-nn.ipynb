{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a simple numpy-based NN to classify MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import numpy as np\n",
    "\n",
    "#TODO use raw mnist data no torch modules\n",
    "mnist_train = dsets.MNIST(root='./mnist', download=True)\n",
    "x_train = np.array(mnist_train.data)\n",
    "y_train = np.array(mnist_train.targets)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='./mnist', train=False, download=True)\n",
    "x_test = np.array(mnist_test.data)\n",
    "y_test = np.array(mnist_test.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_train.shape[0] ==  y_train.shape[0]\n",
    "assert x_test.shape[0] ==  y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "The outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that we can normalize the variance of each neuron’s output to $ 1 $ by scaling its weight vector by the square root of its fan-in (i.e. its number of inputs). That is, the recommended heuristic is to initialize each neuron’s weight vector as: w = np.random.randn(n) / sqrt(n), where $ n $ is the number of its inputs. This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence.\n",
    "\n",
    "The sketch of the derivation is as follows: Consider the inner product $ s = \\sum_i^n w_i x_i $ between the weights $ w $ and input $ x $, which gives the raw activation of a neuron before the non-linearity. We can examine the variance of $ s $:\n",
    "\n",
    "where in the first 2 steps we have used properties of variance. In third step we assumed zero mean inputs and weights, so $ E[x_i] = E[w_i] = 0 $. Note that this is not generally the case: For example ReLU units will have a positive mean. In the last step we assumed that all $ w_i $, $ x_i $ are identically distributed. From this derivation we can see that if we want s to have the same variance as all of its inputs $ x $, then during initialization we should make sure that the variance of every weight $ w $ is $ 1/n $. And since $ \\text{Var}(aX) = a^2\\text{Var}(X) $ for a random variable $ X $ and a scalar $ a $, this implies that we should draw from unit gaussian and then scale it by $ a = \\sqrt{1/n} $, to make its variance $ 1/n $. This gives the initialization w = np.random.randn(n) / sqrt(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 10\n",
    "input_n = 784 # (28 x 28)\n",
    "hidden_n = 64\n",
    "\n",
    "params = {\"W1\": np.random.randn(hidden_n, input_n) * np.sqrt(1. / input_n),\n",
    "          \"b1\": np.zeros((hidden_n, 1)) * np.sqrt(1. / input_n),\n",
    "          \"W2\": np.random.randn(classes, hidden_n) * np.sqrt(1. / hidden_n),\n",
    "          \"b2\": np.zeros((classes, 1)) * np.sqrt(1. / hidden_n)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "The sigmoid function gives an ‘S’ shaped curve. This curve has a finite limit of:\n",
    "\n",
    "‘0’ as $ x $ approaches $ - \\infty $\n",
    "‘1’ as $ x $ approaches $ + \\infty $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x153103990>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-20., 20., 0.2)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "What we use in this example is cross-entropy loss.\n",
    "After averaging over a training set of $ m $ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Y, Y_hat):\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step, Forward pass and back propagation\n",
    "\n",
    "#### Forward pass\n",
    "The forward pass on a single example $x$ executes the following computation on each layer of Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, params):\n",
    "    \"\"\"\n",
    "    feed forward network: 2layer neural net\n",
    "    inputs:\n",
    "        params: dictionay a dictionary contains all the weights and biases\n",
    "    return:\n",
    "        cache: dictionay a dictionary contains all the fully connected units and activations\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "\n",
    "    # Z1 = W1.dot(x) + b1\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "\n",
    "    # A1 = sigmoid(Z1)\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "\n",
    "    # Z2 = W2.dot(A1) + b2\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "\n",
    "    # A2 = softmax(Z2)\n",
    "    cache[\"A2\"] = softmax(cache[\"Z2\"]) # np.exp() / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back Propagation\n",
    "For backpropagation, we’ll need to know how loss changes with respect to each component $w_j$ of $w$. \n",
    "That is, we must compute each $ \\frac{\\partial L}{\\partial w_j} $.\n",
    "\n",
    "Back propagation is actually a fancy name of chain rules. \n",
    "\n",
    "Let us focusing on $ \\frac{\\partial L}{\\partial \\hat{y}} $ first:\n",
    "TODO ADDD\n",
    "\n",
    "By the chain rule, we can get:\n",
    "TODO ADDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(X, Y, params, cache, m_batch):\n",
    "    \"\"\"\n",
    "    back propagation\n",
    "\n",
    "    inputs:\n",
    "        params: dictionay a dictionary contains all the weights and biases\n",
    "        cache: dictionay a dictionary contains all the fully connected units and activations\n",
    "\n",
    "    return:\n",
    "        grads: dictionay a dictionary contains the gradients of corresponding weights and biases\n",
    "    \"\"\"\n",
    "    # error at last layer\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "\n",
    "    # gradients at last layer (Py2 need 1. to transform to float)\n",
    "    dW2 = (1. / m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1. / m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    # back propgate through first layer\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "\n",
    "    # gradients at first layer (Py2 need 1. to transform to float)\n",
    "    dW1 = (1. / m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1. / m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Hyper parameter setup\n",
    "Training process can be simplified as a loop forward pass -> compute loss -> back propagation -> update weights and bias -> forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    # shuffle training set\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[:, permutation]\n",
    "\n",
    "    for j in range(batches):\n",
    "        # get mini-batch\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        # forward and backward\n",
    "        cache = feed_forward(X, params)\n",
    "        grads = back_propagate(X, Y, params, cache, m_batch)\n",
    "\n",
    "        # with momentum \n",
    "        dW1 = (momentum * dW1 + (1. - momentum) * grads[\"dW1\"])\n",
    "        db1 = (momentum * db1 + (1. - momentum) * grads[\"db1\"])\n",
    "        dW2 = (momentum * dW2 + (1. - momentum) * grads[\"dW2\"])\n",
    "        db2 = (momentum * db2 + (1. - momentum) * grads[\"db2\"])\n",
    "\n",
    "        # gradient descent\n",
    "        params[\"W1\"] = params[\"W1\"] - lr * dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - lr * db1\n",
    "        params[\"W2\"] = params[\"W2\"] - lr * dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - lr * db2\n",
    "\n",
    "    # forward pass on training set\n",
    "    cache = feed_forward(X_train, params)\n",
    "    train_loss = loss(Y_train, cache[\"A2\"])\n",
    "\n",
    "    # forward pass on test set\n",
    "    cache = feed_forward(X_test, params)\n",
    "    test_loss = loss(Y_test, cache[\"A2\"])\n",
    "    print(\"Epoch {}: training loss = {}, test loss = {}\".format(\n",
    "        i + 1, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Towards datascience post](https://towardsdatascience.com/building-an-artificial-neural-network-using-pure-numpy-3fe21acc5815)\n",
    "- [Neural net from scratch](https://zhenye-na.github.io/2018/09/09/build-neural-network-with-mnist-from-scratch.html)\n",
    "- [How backpropagation works](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gantutorial",
   "language": "python",
   "name": "gantutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
