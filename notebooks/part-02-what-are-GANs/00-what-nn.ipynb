{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a simple numpy-based NN to classify MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# load MNIST data\n",
    "MNIST_data = h5py.File(\"../../data/MNIST/MNISTdata.hdf5\", 'r')\n",
    "x_train = np.float32(MNIST_data['x_train'][:])\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:, 0])).reshape(-1, 1)\n",
    "x_test  = np.float32(MNIST_data['x_test'][:])\n",
    "y_test  = np.int32(np.array(MNIST_data['y_test'][:, 0])).reshape(-1, 1)\n",
    "MNIST_data.close()\n",
    "\n",
    "# stack together for next step\n",
    "X = np.vstack((x_train, x_test))\n",
    "y = np.vstack((y_train, y_test))\n",
    "\n",
    "# one-hot encoding\n",
    "digits = 10\n",
    "examples = y.shape[0]\n",
    "y = y.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "# number of training set\n",
    "m = 60000\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:, :m], Y_new[:, m:]\n",
    "\n",
    "# shuffle training set\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (10, 60000), (784, 10000), (10, 10000))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape[1] ==  Y_train.shape[1]\n",
    "assert X_test.shape[1] ==  Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "The outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that we can normalize the variance of each neuron’s output to $1$ by scaling its weight vector by the square root of its number of inputs. \n",
    "\n",
    "That is, the recommended heuristic is to initialize each neuron’s weight vector as: \n",
    "$w = \\frac{\\mathbb{N}(n)} {\\sqrt(n)}$, where $ n $ is the number of its inputs.\n",
    "\n",
    "This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 10\n",
    "input_n = 784 # (28 x 28)\n",
    "hidden_n = 64\n",
    "\n",
    "params = {\"W1\": np.random.randn(hidden_n, input_n) * np.sqrt(1. / input_n),\n",
    "          \"b1\": np.zeros((hidden_n, 1)) * np.sqrt(1. / input_n),\n",
    "          \"W2\": np.random.randn(classes, hidden_n) * np.sqrt(1. / hidden_n),\n",
    "          \"b2\": np.zeros((classes, 1)) * np.sqrt(1. / hidden_n)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "This curve has a finite limit of:\n",
    "\n",
    "‘0’ as $ x $ approaches $ - \\infty $\n",
    "‘1’ as $ x $ approaches $ + \\infty $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbfa629dc18>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-20., 20., 0.2)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "What we use in this example is cross-entropy loss.\n",
    "After averaging over a training set of $ m $ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Y, Y_hat):\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step, Forward pass and back propagation\n",
    "\n",
    "#### Forward pass\n",
    "The forward pass on a single example $x$ executes the following computation on each layer of Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, params):\n",
    "    \"\"\"\n",
    "    feed forward network: 2layer neural net\n",
    "    inputs:\n",
    "        params: dictionary contains all the weights and biases\n",
    "    return:\n",
    "        cache: a dictionary contains all the fully connected units and activations\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "\n",
    "    # Z1 = W1.dot(x) + b1\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "\n",
    "    # A1 = sigmoid(Z1)\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "\n",
    "    # Z2 = W2.dot(A1) + b2\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "\n",
    "    # A2 = softmax(Z2)\n",
    "    cache[\"A2\"] = softmax(cache[\"Z2\"])\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back Propagation\n",
    "Back propagation is actually a fancy name of chain rules. \n",
    "\n",
    "Backpropagation is based around four fundamental equations. Together, those equations give us a way of computing both the error $\\delta^l$ and the gradient of the cost function.\n",
    "\n",
    "An equation for the error in the output layer, $\\delta^L$:\n",
    "\\begin{eqnarray} \n",
    "  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\n",
    "\\tag{BP1}\\end{eqnarray}\n",
    "\n",
    "An equation for the error $\\delta^l$ in terms of the error in the next layer, $\\delta^{l+1}$:\n",
    "\\begin{eqnarray} \n",
    "  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\n",
    "\\tag{BP2}\\end{eqnarray}\n",
    "\n",
    "An equation for the rate of change of the cost with respect to any bias in the network:\n",
    "\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\n",
    "  \\delta^l_j.\n",
    "\\tag{BP3}\\end{eqnarray}\n",
    "\n",
    "An equation for the rate of change of the cost with respect to any weight in the network:\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\n",
    "\\tag{BP4}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>How backpropagation works:</b> Check out http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "    <b>Great introduction with intuitive interpretation!</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(X, Y, params, cache, m_batch):\n",
    "    \"\"\"\n",
    "    back propagation\n",
    "\n",
    "    inputs:\n",
    "        params: dictionary contains all the weights and biases\n",
    "        cache: dictionary contains all the fully connected units and activations\n",
    "\n",
    "    return:\n",
    "        grads: dictionary contains the gradients of corresponding weights and biases\n",
    "    \"\"\"\n",
    "    # error at last layer\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "\n",
    "    # gradients at last layer (Py2 need 1. to transform to float)\n",
    "    dW2 = (1. / m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1. / m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    # back propagate through first layer\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "\n",
    "    # gradients at first layer (Py2 need 1. to transform to float)\n",
    "    dW1 = (1. / m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1. / m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Hyper parameter setup\n",
    "Training process can be simplified as a loop forward pass -> compute loss -> back propagation -> update weights and bias -> forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "batches = int(X_train.shape[1]/64)\n",
    "history_loss_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 1.623204380193469, test loss = 1.6113645849504843\n",
      "Epoch 2: training loss = 1.0959083488410803, test loss = 1.0781490074095392\n",
      "Epoch 3: training loss = 0.8266707391291508, test loss = 0.8079428792725178\n",
      "Epoch 4: training loss = 0.6824468154255389, test loss = 0.6638398415483551\n",
      "Epoch 5: training loss = 0.5946645526027842, test loss = 0.5762846175601509\n",
      "Epoch 6: training loss = 0.5360955586482143, test loss = 0.5180131832685556\n",
      "Epoch 7: training loss = 0.49410086367875, test loss = 0.47631394996472376\n",
      "Epoch 8: training loss = 0.46238275985059424, test loss = 0.44460446112276836\n",
      "Epoch 9: training loss = 0.4381516311528761, test loss = 0.42087858725859145\n",
      "Epoch 10: training loss = 0.41852581519452364, test loss = 0.4016330446851611\n",
      "Epoch 11: training loss = 0.4023785909182574, test loss = 0.3858861640289548\n",
      "Epoch 12: training loss = 0.3891534397888373, test loss = 0.37307861117976704\n",
      "Epoch 13: training loss = 0.3775934111408165, test loss = 0.36232356529689125\n",
      "Epoch 14: training loss = 0.36772712286599335, test loss = 0.3526641770576827\n",
      "Epoch 15: training loss = 0.3591008407370659, test loss = 0.3449188951873443\n",
      "Epoch 16: training loss = 0.351566992555923, test loss = 0.33733153755875794\n",
      "Epoch 17: training loss = 0.34459859536126075, test loss = 0.33115725474301466\n",
      "Epoch 18: training loss = 0.3385478963097311, test loss = 0.32545507129499374\n",
      "Epoch 19: training loss = 0.33272230970119615, test loss = 0.32004451200808715\n",
      "Epoch 20: training loss = 0.3274815878703693, test loss = 0.3150559292206987\n",
      "Epoch 21: training loss = 0.3228704886551837, test loss = 0.31066309478658044\n",
      "Epoch 22: training loss = 0.3182691994788282, test loss = 0.30661651499180476\n",
      "Epoch 23: training loss = 0.3139994434091194, test loss = 0.3025475112905915\n",
      "Epoch 24: training loss = 0.31025789488523114, test loss = 0.299496782436225\n",
      "Epoch 25: training loss = 0.3064060623587297, test loss = 0.29574636896859485\n",
      "Epoch 26: training loss = 0.3028955771171927, test loss = 0.29281432854858086\n",
      "Epoch 27: training loss = 0.2995746514602135, test loss = 0.28951437704322547\n",
      "Epoch 28: training loss = 0.29643285131422675, test loss = 0.2869070049036613\n",
      "Epoch 29: training loss = 0.29349748376851126, test loss = 0.28426889977130776\n",
      "Epoch 30: training loss = 0.2905199089596962, test loss = 0.2814574427037577\n",
      "Epoch 31: training loss = 0.28766773784147237, test loss = 0.278578039998825\n",
      "Epoch 32: training loss = 0.28486736265576856, test loss = 0.27647176611389923\n",
      "Epoch 33: training loss = 0.2822185920834213, test loss = 0.2742530447527963\n",
      "Epoch 34: training loss = 0.27965136640114424, test loss = 0.27202310283795256\n",
      "Epoch 35: training loss = 0.2772327501225555, test loss = 0.26958672836673997\n",
      "Epoch 36: training loss = 0.27474085799177717, test loss = 0.2674394546615231\n",
      "Epoch 37: training loss = 0.2723886695137012, test loss = 0.26528132949066036\n",
      "Epoch 38: training loss = 0.2701085160307339, test loss = 0.26313920144440905\n",
      "Epoch 39: training loss = 0.267925097326937, test loss = 0.2616066430245368\n",
      "Epoch 40: training loss = 0.2656765892006827, test loss = 0.25926969263254057\n",
      "Epoch 41: training loss = 0.26346215007999446, test loss = 0.257311232656733\n",
      "Epoch 42: training loss = 0.26150167725624096, test loss = 0.2553084485247569\n",
      "Epoch 43: training loss = 0.25931234499426487, test loss = 0.25377159747845296\n",
      "Epoch 44: training loss = 0.25727299371689033, test loss = 0.2519615799108455\n",
      "Epoch 45: training loss = 0.2552794591959232, test loss = 0.25041149594926354\n",
      "Epoch 46: training loss = 0.2533532004249513, test loss = 0.24841083346130738\n",
      "Epoch 47: training loss = 0.2515231859821616, test loss = 0.24693088494183846\n",
      "Epoch 48: training loss = 0.24952348503363142, test loss = 0.24519265280261154\n",
      "Epoch 49: training loss = 0.24767712757542956, test loss = 0.2432790698598587\n",
      "Epoch 50: training loss = 0.24570913732533467, test loss = 0.24181748837941774\n",
      "Epoch 51: training loss = 0.2438496431102163, test loss = 0.2399088584100252\n",
      "Epoch 52: training loss = 0.24205667640201256, test loss = 0.23824703754944865\n",
      "Epoch 53: training loss = 0.2402420102423933, test loss = 0.23674048264346562\n",
      "Epoch 54: training loss = 0.23853895501323605, test loss = 0.23530448653516883\n",
      "Epoch 55: training loss = 0.2368688234350297, test loss = 0.23358046482125658\n",
      "Epoch 56: training loss = 0.23509428532912008, test loss = 0.2321120967103418\n",
      "Epoch 57: training loss = 0.23371680707461293, test loss = 0.2310780066818638\n",
      "Epoch 58: training loss = 0.2317293502893233, test loss = 0.22894884860576614\n",
      "Epoch 59: training loss = 0.23016501212226528, test loss = 0.22754922899718577\n",
      "Epoch 60: training loss = 0.22853734487427768, test loss = 0.22627095902966043\n",
      "Epoch 61: training loss = 0.22707427031272218, test loss = 0.22457920381738264\n",
      "Epoch 62: training loss = 0.22556975824662254, test loss = 0.22342348245175897\n",
      "Epoch 63: training loss = 0.22394016978587003, test loss = 0.22214378451809072\n",
      "Epoch 64: training loss = 0.22240119884035103, test loss = 0.22006510025665185\n",
      "Epoch 65: training loss = 0.22080130371365642, test loss = 0.2191556871375896\n",
      "Epoch 66: training loss = 0.21934927095481174, test loss = 0.21750881442399947\n",
      "Epoch 67: training loss = 0.2178467928192313, test loss = 0.21627573863046223\n",
      "Epoch 68: training loss = 0.21656956422352497, test loss = 0.21533207396738813\n",
      "Epoch 69: training loss = 0.21508849204486555, test loss = 0.21372939943445154\n",
      "Epoch 70: training loss = 0.21370827505579276, test loss = 0.21225001599296872\n",
      "Epoch 71: training loss = 0.21225320443110343, test loss = 0.21144524543831658\n",
      "Epoch 72: training loss = 0.2108885002974858, test loss = 0.2102776562763972\n",
      "Epoch 73: training loss = 0.20954210064800713, test loss = 0.2087756764993586\n",
      "Epoch 74: training loss = 0.208194835808494, test loss = 0.20761550860818484\n",
      "Epoch 75: training loss = 0.20706684437003592, test loss = 0.20648573570030354\n",
      "Epoch 76: training loss = 0.205624963775563, test loss = 0.205096640239443\n",
      "Epoch 77: training loss = 0.20438653907990292, test loss = 0.20426750502243504\n",
      "Epoch 78: training loss = 0.20311977679491527, test loss = 0.2027800387908222\n",
      "Epoch 79: training loss = 0.20182033645409317, test loss = 0.2017654254690189\n",
      "Epoch 80: training loss = 0.2005871946954701, test loss = 0.20055084601284986\n",
      "Epoch 81: training loss = 0.19939827519673903, test loss = 0.1995667893531613\n",
      "Epoch 82: training loss = 0.19824643291696187, test loss = 0.19856482845564033\n",
      "Epoch 83: training loss = 0.19701123703709778, test loss = 0.1974063086370404\n",
      "Epoch 84: training loss = 0.1959129722558054, test loss = 0.19624650081959166\n",
      "Epoch 85: training loss = 0.1947552009834779, test loss = 0.19510139015966296\n",
      "Epoch 86: training loss = 0.19358303338358793, test loss = 0.1941810905892786\n",
      "Epoch 87: training loss = 0.19242753647297092, test loss = 0.19303890874966217\n",
      "Epoch 88: training loss = 0.19135238457346537, test loss = 0.1922042558615807\n",
      "Epoch 89: training loss = 0.1902226439919596, test loss = 0.1910884036623716\n",
      "Epoch 90: training loss = 0.18917133039844, test loss = 0.1900239652532282\n",
      "Epoch 91: training loss = 0.18816289772771091, test loss = 0.18923456213216158\n",
      "Epoch 92: training loss = 0.1870551522737503, test loss = 0.18825794740338042\n",
      "Epoch 93: training loss = 0.18611841619640812, test loss = 0.18767239229974655\n",
      "Epoch 94: training loss = 0.1850060904345, test loss = 0.18645013092181048\n",
      "Epoch 95: training loss = 0.18391732556251747, test loss = 0.18539911622916957\n",
      "Epoch 96: training loss = 0.18298730458387677, test loss = 0.18439692364437568\n",
      "Epoch 97: training loss = 0.18201993841394323, test loss = 0.18388391986140576\n",
      "Epoch 98: training loss = 0.18105105508679187, test loss = 0.18281725473643523\n",
      "Epoch 99: training loss = 0.1799979375810235, test loss = 0.18172077248659857\n",
      "Epoch 100: training loss = 0.1790280688337282, test loss = 0.18093950914387927\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    # shuffle training set\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    x_train_shuffled = X_train[:, permutation]\n",
    "    y_train_shuffled = Y_train[:, permutation]\n",
    "\n",
    "    for j in range(batches):\n",
    "        # get mini-batch\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "        X = x_train_shuffled[:, begin:end]\n",
    "        Y = y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        # forward and backward\n",
    "        cache = feed_forward(X, params)\n",
    "        grads = back_propagate(X, Y, params, cache, m_batch)\n",
    "\n",
    "        # gradient descent\n",
    "        params[\"W1\"] = params[\"W1\"] - lr * grads[\"dW1\"] # dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - lr * grads[\"db1\"] #db1\n",
    "        params[\"W2\"] = params[\"W2\"] - lr * grads[\"dW2\"] #dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - lr * grads[\"db2\"] #db2\n",
    "\n",
    "    # forward pass on training set\n",
    "    cache = feed_forward(X_train, params)\n",
    "    train_loss = loss(Y_train, cache[\"A2\"])\n",
    "    history_loss_train.append(train_loss)\n",
    "    # forward pass on test set\n",
    "    cache = feed_forward(X_test, params)\n",
    "    test_loss = loss(Y_test, cache[\"A2\"])\n",
    "    print(\"Epoch {}: training loss = {}, test loss = {}\".format(\n",
    "        i + 1, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbfa6037c50>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAHd1JREFUeJzt3XmUXGd55/HvU2t3VXerV7VlLZYsyzZeMHbaG0vsBBhkM2NPDhAQEOLEoMycEJgJ+8kMJPBPWCYJzBgYxTiOORMb2+GAAIPDIsck2EZtjIVl2dZiWWptvaml3quXZ/641a1Sq7urJFWrum79Puf06ap73656rq/8e2+999Z7zd0REZFwiZS6ABERKT6Fu4hICCncRURCSOEuIhJCCncRkRBSuIuIhJDCXUQkhBTuIiIhpHAXEQmhWKneuLm52VevXl2qtxcRKUtPP/10t7u35GtXsnBfvXo17e3tpXp7EZGyZGavFNJOwzIiIiGkcBcRCSGFu4hICCncRURCSOEuIhJCCncRkRBSuIuIhFDZhfuLh/v50qMv0jMwWupSREQWrbzhbmb3mFmnmT03T5ubzezXZrbdzP61uCWebE/XAP9nyy46+xXuIiJzKeTI/V5g/Vwrzawe+Cpwm7tfDryjOKXNrjoRBWAoM7GQbyMiUtbyhru7Pw70ztPk3cC33X1ftn1nkWqbVXU8CPdhhbuIyJyKMeZ+MdBgZo+Z2dNm9r65GprZRjNrN7P2rq6uM3qzVCKYDmcoM35Gfy8iUgmKEe4x4LeAtwJvAf6nmV08W0N33+Tube7e1tKSd1KzWU0NywyP6chdRGQuxZgVsgPocfdBYNDMHgeuAl4qwmufIqUxdxGRvIpx5P5d4PVmFjOzFHA9sKMIrzsrhbuISH55j9zN7H7gZqDZzDqAzwBxAHf/urvvMLMfAduASeBud5/zssmzNT0sozF3EZE55Q13d99QQJsvAl8sSkV5JKIRohHTkbuIyDzK7huqZkYqHtUJVRGReZRduEMwNKPr3EVE5laW4Z5KRDUsIyIyj7IM96q4wl1EZD5lGe6pRJThMV0tIyIylzIN95iO3EVE5lGW4a4TqiIi8yvLcNcJVRGR+SncRURCqCzDvToe0/QDIiLzKMtwD66WmcDdS12KiMiiVJbhXp2IMukwOj5Z6lJERBalsgz3VEK32hMRmU9ZhvvUfVSHNHmYiMisyjPcNae7iMi8yjLcT9wkW0fuIiKzKdNw1632RETmU5bhXq0TqiIi88ob7mZ2j5l1mtm890U1s2vNbNzM3l688manI3cRkfkVcuR+L7B+vgZmFgU+D/xLEWrKKxWfGnPXCVURkdnkDXd3fxzozdPsz4B/BjqLUVQ+U8MyI7oUUkRkVmc95m5my4HfA7529uUURsMyIiLzK8YJ1b8DPuHueecCMLONZtZuZu1dXV1n/IbTX2JSuIuIzCpWhNdoAx4wM4Bm4FYzG3f378xs6O6bgE0AbW1tZzzrVyRiJGMRhjUsIyIyq7MOd3dfM/XYzO4Fvj9bsBdbMKe7TqiKiMwmb7ib2f3AzUCzmXUAnwHiAO7+9QWtbh66j6qIyNzyhru7byj0xdz9jrOq5jToPqoiInMry2+ogm61JyIyn7IN9+q4jtxFROZStuGeSkQZGtMJVRGR2ZRxuOuEqojIXMo23KsTUUYU7iIisyrbcA+GZRTuIiKzKdtwr47rahkRkbmUb7gnomTGJ5mYPONZDEREQqtsw/3EzJC6YkZEZKayDffq7E2yda27iMipyjbcU5r2V0RkTuUb7rphh4jInMo23KdutTesb6mKiJyibMM9lZi6SbaO3EVEZirjcM8euSvcRUROUbbhfmJYRuEuIjJT+Ya7rpYREZlT2Ya7rpYREZlb2Yb79LCMvqEqInKKvOFuZveYWaeZPTfH+veY2TYz+42Z/cLMrip+madKRCNEI6YjdxGRWRRy5H4vsH6e9S8DN7n7lcDngE1FqCsvMyOlmSFFRGYVy9fA3R83s9XzrP9FztMngRVnX1ZhqhO6j6qIyGyKPeZ+J/DDuVaa2UYzazez9q6urrN+M92wQ0RkdkULdzP7HYJw/8Rcbdx9k7u3uXtbS0vLWb9ndSKmE6oiIrPIOyxTCDN7NXA3cIu79xTjNQuRSmjMXURkNmd95G5mq4BvA3/g7i+dfUmFSyWi+oaqiMgs8h65m9n9wM1As5l1AJ8B4gDu/nXg00AT8FUzAxh397aFKjhXVTxKV//ouXgrEZGyUsjVMhvyrH8/8P6iVXQaNCwjIjK7sv2GKijcRUTmUtbhXh3X1TIiIrMp63Cfus7d3UtdiojIolLW4V6diOIOo+OTpS5FRGRRKetw17S/IiKzC0m4a9xdRCRXWYd7dfYm2Zo8TETkZGUd7qm47qMqIjKb8g73ZBDuAyMalhERyVXW4d6YTgDQO5QpcSUiIotLWYd7UzoJQO+gwl1EJFdZh3tDKg5A94DCXUQkV1mHeywaoT4Vp3dQM0OKiOQq63AHaEonNCwjIjJDCMI9qWEZEZEZyj7cG3XkLiJyirIP96YahbuIyEzlH+7pBEeHMkxMatpfEZEpecPdzO4xs04ze26O9WZmXzGzXWa2zcyuKX6Zc2uqSeIOR/VFJhGRaYUcud8LrJ9n/S3AuuzPRuBrZ19W4aa/paqhGRGRaXnD3d0fB3rnaXI7cJ8HngTqzWxZsQrMpykb7t0DutZdRGRKMcbclwP7c553ZJedE001moJARGSmc3pC1cw2mlm7mbV3dXUV5TWnhmV6dK27iMi0YoT7AWBlzvMV2WWncPdN7t7m7m0tLS1FeOsT88v06MhdRGRaMcJ9M/C+7FUzNwDH3P1QEV63ILFohAbNLyMicpJYvgZmdj9wM9BsZh3AZ4A4gLt/HXgEuBXYBQwBf7RQxc6lMZ3QsIyISI684e7uG/Ksd+BPi1bRGWhKJzUsIyKSo+y/oQqagkBEZKZQhHswLKMxdxGRKaEI96Z0gr7hMc0vIyKSFY5w1/wyIiInCUW464tMIiInC0W4N9Vkw13XuouIAGEJ97TmlxERyRWKcNewjIjIyUIR7g2pOGaaX0ZEZEoowj0WjVBfrfllRESmhCLcQfPLiIjkCk24N9VofhkRkSnhCXdNQSAiMi004d6Y1uRhIiJTQhPuTTVJ+obHGJ+YLHUpIiIlF55wTyey88uMlboUEZGSC024T32RSUMzIiIhCnfNLyMickJB4W5m683sRTPbZWafnGX9KjPbYmbPmNk2M7u1+KXOb2ltFQCHj42c67cWEVl08oa7mUWBu4BbgMuADWZ22Yxm/wN40N2vBt4FfLXYheazqjFFNGLs6Ro8128tIrLoFHLkfh2wy933uHsGeAC4fUYbB+qyj5cAB4tXYmESsQgXNKbY3TVwrt9aRGTRiRXQZjmwP+d5B3D9jDZ/CfyLmf0ZkAbeVJTqTtOFLTUKdxERindCdQNwr7uvAG4Fvmlmp7y2mW00s3Yza+/q6irSW5+wdmmavd1DutZdRCpeIeF+AFiZ83xFdlmuO4EHAdz9CaAKaJ75Qu6+yd3b3L2tpaXlzCqex9rmGjITk3QcHS76a4uIlJNCwn0rsM7M1phZguCE6eYZbfYBbwQws1cRhHvxD83zWLs0DcCebg3NiEhlyxvu7j4OfBB4FNhBcFXMdjP7rJndlm32EeADZvYscD9wh7v7QhU9lwubawDY3akrZkSkshVyQhV3fwR4ZMayT+c8fh54XXFLO30N6QRN6YROqopIxQvNN1SnrNUVMyIiIQz3pWl264tMIlLhwhfuLTX0DmY4qgnERKSChS7cL2zRFTMiIqEL97UtumJGRCR04b6iIUUiGtFJVRGpaKEL92jEWNOcVriLSEULXbiDrpgREQlnuLfUsK93iMy4JhATkcoU2nCfmHT29eroXUQqU2jDHWBXp8bdRaQyhTLc17XWEI8az+zrK3UpIiIlEcpwr4pHuXpVA0/s6Sl1KSIiJRHKcAe48cImnjtwjGPDY6UuRUTknAttuL92bROTDr98ubfUpYiInHOhDffXrKonGYvwi93dpS5FROScC224J2NRrl3dyBO7Ne4uIpUntOEOcOPaJl443E/PwGipSxEROadCH+4AT2ncXUQqTEHhbmbrzexFM9tlZp+co83vm9nzZrbdzP6puGWemSuXLyGdiGrcXUQqTt4bZJtZFLgLeDPQAWw1s83Zm2JPtVkHfAp4nbsfNbOlC1Xw6YhHI1y3RuPuIlJ5Cjlyvw7Y5e573D0DPADcPqPNB4C73P0ogLt3FrfMM3fj2iZ2dw1y5PhIqUsRETlnCgn35cD+nOcd2WW5LgYuNrN/N7MnzWz9bC9kZhvNrN3M2ru6us6s4tP02rXNADp6F5GKUqwTqjFgHXAzsAH4ezOrn9nI3Te5e5u7t7W0tBTpref3qmV1tNYl+d6zB8/J+4mILAaFhPsBYGXO8xXZZbk6gM3uPubuLwMvEYR9yUUjxtuuWcGWFzs1NCMiFaOQcN8KrDOzNWaWAN4FbJ7R5jsER+2YWTPBMM2eItZ5Vt7RtpJJh2//amafJCISTnnD3d3HgQ8CjwI7gAfdfbuZfdbMbss2exToMbPngS3Ax9x90Qxyr2lOc93qRh5q34+7l7ocEZEFl/dSSAB3fwR4ZMayT+c8duDPsz+L0jvaVvCxh7fx9CtHaVvdWOpyREQWVKi/oZrr1iuXkU5E+dbW/fkbi4iUuYoJ93Qyxn+66nx+8JtDDIyOl7ocEZEFVTHhDsGJ1aHMBN/XZZEiEnIVFe7XrKrnsmV1fPWx3WTGJ0tdjojIgqmocDczPrb+Evb1DvHA1n2lLkdEZMFUVLgD3HxxC9evaeQrP93JoMbeRSSkKi7czYxP3HIp3QMZvvFvL5e6HBGRBVFx4Q5wzaoG3nJ5K5se36O7NIlIKFVkuAN87C2XMJQZ53/9+KVSlyIiUnQVG+4XLa3lj1+3hn96ah9bXlg008+LiBRFxYY7wEffcgmXnlfLxx5+lm4Nz4hIiFR0uFfFo3xlw9UcHxnn4w9v06RiIhIaFR3uABe31vKpWy7lZy90ct8Tr5S6HBGRoqj4cAe447WreeOlS/mr723nR88dLnU5IiJnTeFOcO37/3731bxmZT0fuv8Z/n1Xd6lLEhE5Kwr3rFQixj13XMua5jQb72vn1/v7Sl2SiMgZU7jnqE8l+Oad19FUk+S9dz/Fz3d2lbokEZEzonCfYWldFd/6kxtY0VDNH/3DVh5s1809RKT8FBTuZrbezF40s11m9sl52r3NzNzM2opX4rm3bEk1D/2XG7lxbRMff3gbX3z0BcYnNEWwiJSPvOFuZlHgLuAW4DJgg5ldNku7WuDDwFPFLrIUaqvi3HPHtbyzbSV3bdnNhr9/ko6jQ6UuS0SkIIUcuV8H7HL3Pe6eAR4Abp+l3eeAzwMjRayvpOLRCJ9/+6v523dexY5D/dzy5Z/z3V8f0JedRGTRKyTclwO5A88d2WXTzOwaYKW7/6CItS0av3f1Ch750Bu4aGkNH37g17zn7qd46Uh/qcsSEZnTWZ9QNbMI8DfARwpou9HM2s2svaurvK5EWdWU4qE/uZHP3X452w8e55Yv/5y/3Lxdc9KIyKJUSLgfAFbmPF+RXTalFrgCeMzM9gI3AJtnO6nq7pvcvc3d21paWs686hKJRSP8wY2r2fLRm3nntSu574m9/PYXtvCFH71A31Cm1OWJiEyzfOPHZhYDXgLeSBDqW4F3u/v2Odo/BnzU3dvne922tjZvb5+3yaK3u2uAL/9kJ9/bdpBUPMrvX7uSO167mgua0qUuTURCysyedve8VyTmPXJ393Hgg8CjwA7gQXffbmafNbPbzr7U8rW2pYavbLiaH374DfyHy8/jm0+8ws1feowP3NfOlhc6dfmkiJRM3iP3hRKGI/eZjhwf4ZtPvML9v9xHz2CG1rokb7tmBbe95nwuaa3FzEpdooiUuUKP3BXuCyAzPsnPXjjCg+0dPPZiJ5MOa1vSvPXKZbzlivO4bFmdgl5EzojCfZHo6h/lR9sP84NtB3nq5V7cYXl9NW++rJWbLmnh+jWNpBKxUpcpImVC4b4IdQ+M8tMdR/jx80f4+c5uRscnSUQjXHNBPa9b28xrL2ri1SvqiUc15Y+IzE7hvsiNjE2wdW8v/7azm5/v7Ob5Q8cBSCei/NbqRtouaKBtdQNXragnndSRvYgEFO5lpncww5N7evjF7m62vnyUlzr7cYeIwUVLa7hyeT1XrVzClcuX8KpldVTFo6UuWURKQOFe5o4NjfGrfUd5tqOPbR3H2NbRR/dA8EWpWMRY11rL5efXcfn5dVx6Xh0Xt9bQVJMscdUistAU7iHj7hw6NsK2jj6e7TjG9oPHef7gsenAB2hKJ1jXWsMlrbVcfF4tF7fWsralhsZ0ooSVi0gxFRruGswtE2bG+fXVnF9fzforlgFB4Hf2j/LC4X52Huln55EBXjzSz8NPdzCYmZj+24ZUnAtbariwOc2aljQXNqdZ1ZjmgqaUxvNFQkr/Z5cxM6O1rorWuipuuvjEXD3uzoG+YXYeGWB31wC7uwbZ3TXAv77UxUNPd5z0Gs01SVY1VrOyMcWqxtRJv8+rqyIa0fX4IuVI4R5CZsaKhhQrGlL8zqVLT1o3MDrO3u5BXukZYm/PIK/0DLK/d5j2vUf53rMHmcwZpYtHg08LKxqqWdmQYkVDNcsbqllen+L8+irOq6sipss2RRYlhXuFqUnGuGL5Eq5YvuSUdWMTkxzsG2Z/7zCv9A7ScXSYjqPD7O8d4ic7Ok+Z3jhi0FpXxbIlVZxfX82yJVWct2Tqd7C8pSapDkCkBBTuMi0ejXBBU5oLmtK8nuZT1g9nJjjQN8TBvhEO9g1zoG94+vFvDhzjx88fYXT85MnSohGjpSZJa12S1rog9Fvrqlham2RpXRWtdUmW1lbRkIprSgaRIlK4S8GqE1EuWlrLRUtrZ13v7vQNjXH4+AiHj41w8Ngwh/pGOHx8hCPHR9jbM8iTe3o4PjJ+yt/Go0EnsHQ6+JO01laxtC5Jc02SppokzTUJmmuSusZfpAAKdykaM6MhnaAhneBVy+rmbDcyNkHn8VGO9I/QeXyUzv4RjmR/d/WPsrdnkF/u7aVvaGzWv19SHaelNklLTZLm6d8JmtNJmrIdQJM6AqlwCnc556riUVY1pVjVlJq33ej4BF39o/QMZOgeGKV7YDTbGQQdQfdAJvhyV//oSZd+5kolojSmEzSlEzTVJGlKJ2iuTWafJ2hMB49bapM0phOa10dCQ+Eui1YyFp2+6ief4czEdAfQM5ChZ3CU7oEMvYPBT89ghiPHR9h+8Bg9AxnGJ2f/8t6S6jhN6QSN2Z+mmgQNqRPPp34aUsEnlHQiqnMFsigp3CUUqhNRVmavz89nctLpHxmnZ3CU3sEM3VOdQX/wu2cwQ+9Ahr09gzyzv4+jg3N3BolohCWpOA2p+HQn0JBO0JCKs6Q6Tl1VnLpsh9FUEwwh1VbFiOj7A7LAFO5ScSIRY0kqzpJUnAsLuE+7u3N8ZDz7KSD4ZNA3NMbRoQy9QxmOZR8fHRxjZ+cARwczHB3KMEd/QMSCTwhLquMsSSWmO4agM4hRN7Uu+1OfbbMkFScZ0zkEKUxB4W5m64EvA1Hgbnf/6xnr/xx4PzAOdAF/7O6vFLlWkZIws+mgXdNc2M3PJyedwcw4x0fGOT48Nj1U1NU/yrHhMfqGxugbHqNvKEPPQIZdnQMcGx6jf5YriXKlEtGc0I9TX52gPhv8dVUndwpTPzVVMWqrYuoYKkzecDezKHAX8GagA9hqZpvd/fmcZs8Abe4+ZGb/FfgC8M6FKFikHEQiRm1VnNqqOMvrqwv+u4lJZ2BknGPDY9M/fcPBJ4W+oeB3sCx4vqd7ILtujEyeG7InopHsp4JY8ClhetgoNt0x5C6rrQo+SQTbEdOVR2WmkCP364Bd7r4HwMweAG4HpsPd3bfktH8SeG8xixSpFNGcIaPTNTI2wfGcTmHqp39knIHR4BPE8ZETy3sHM+ztHpz+dDHXeYUpiVhkOuxrkjHSySjpRPCpILdjqKmKUZOMBZ8Ykic6h5qqGDUJnW84VwoJ9+XA/pznHcD187S/E/jh2RQlIqevKh6lKh5laV3Vaf+tuzOUmZjuDI6PjHE8+7h/ZCzoAEamno8zMDLG4OgEh46N8FLnGMeHg/X5ZhA3g5pE7KQOoCYZy3YWselOoyYZn+4cptalk9GcNjFS8ag6inkU9YSqmb0XaANummP9RmAjwKpVq4r51iJyFswsG6BnHgmTk87Q2AT9I2MMjATnGwZGxxnIdhC5HcXgaLBuqrM4fGwkaDsarMvzISJbM6QTM0I/MdVJRKe3J5WITncgqWSMdCJKKhEsTyeDx+lEjFQyGqrvORSyJw8AK3Oer8guO4mZvQn4C+Amdx+duR7A3TcBmyC4WcdpVysii1YkYtMhyqnz0hXM3Rkem5juGKZCf2h0gsHMiQ5gYHQi+D0yzkBmfPpxx9EhBjNB+4HR8VPmO5pPIhaZDv+qeGS6Ezjp00Mi6CSq41FSiSjVieB3KhGlOh478Tj7d6lEjETs3HcahYT7VmCdma0hCPV3Ae/ObWBmVwP/F1jv7p1Fr1JEKoaZZUM1xhzTGJ2W8YlJBjNBRzCUmWAoM85gtmMYGptgOBN0FEOjQScxNDrB8NgEw5mJ6U7i0LERBrN/N5QJXud0xCKW0wnEeM/1q3j/Gy48+42b7z3zNXD3cTP7IPAowaWQ97j7djP7LNDu7puBLwI1wEPZb+vtc/fbFrBuEZGCxKIRllRHWFJ9+iep5zI56YyMTzCUCTqBqU5j+nG20xjKPp/qWKY6jOZzcL/jggbY3P0R4JEZyz6d8/hNRa5LRGTRikROfLpYrMJz9kBERKYp3EVEQkjhLiISQgp3EZEQUriLiISQwl1EJIQU7iIiIaRwFxEJIfN807gt1BubdQFnekOPZqC7iOWUi0rc7krcZqjM7a7EbYbT3+4L3D3vPcRKFu5nw8za3b2t1HWca5W43ZW4zVCZ212J2wwLt90alhERCSGFu4hICJVruG8qdQElUonbXYnbDJW53ZW4zbBA212WY+4iIjK/cj1yFxGReZRduJvZejN70cx2mdknS13PQjCzlWa2xcyeN7PtZvbh7PJGM/uxme3M/m4oda0LwcyiZvaMmX0/+3yNmT2V3effMrNEqWssJjOrN7OHzewFM9thZjdWwr42s/+e/ff9nJndb2ZVYdzXZnaPmXWa2XM5y2bdvxb4Snb7t5nZNWf6vmUV7mYWBe4CbgEuAzaY2WWlrWpBjAMfcffLgBuAP81u5yeBn7r7OuCn2edh9GFgR87zzwN/6+4XAUeBO0tS1cL5MvAjd78UuIpg20O9r81sOfAhoM3dryC4y9u7COe+vhdYP2PZXPv3FmBd9mcj8LUzfdOyCnfgOmCXu+9x9wzwAHB7iWsqOnc/5O6/yj7uJ/iffTnBtv5jttk/Av+5NBUuHDNbAbwVuDv73IDfBR7ONgnVdpvZEuC3gW8AuHvG3fuogH1NcCe4ajOLASngECHc1+7+ONA7Y/Fc+/d24D4PPAnUm9myM3nfcgv35cD+nOcd2WWhZWargauBp4BWdz+UXXUYaC1RWQvp74CPA1O3rG8C+tx9PPs8bPt8DdAF/EN2KOpuM0sT8n3t7geALwH7CEL9GPA04d7Xuebav0XLuHIL94piZjXAPwP/zd2P567z4DKnUF3qZGb/Eeh096dLXcs5FAOuAb7m7lcDg8wYggnpvm4gOEpdA5wPpDl16KIiLNT+LbdwPwCszHm+IrssdMwsThDs/8/dv51dfGTqI1r2d2ep6lsgrwNuM7O9BENuv0swHl2f/egO4dvnHUCHuz+Vff4wQdiHfV+/CXjZ3bvcfQz4NsH+D/O+zjXX/i1axpVbuG8F1mXPqCcITsBsLnFNRZcdZ/4GsMPd/yZn1WbgD7OP/xD47rmubSG5+6fcfYW7rybYtz9z9/cAW4C3Z5uFarvd/TCw38wuyS56I/A8Id/XBMMxN5hZKvvvfWq7Q7uvZ5hr/24G3pe9auYG4FjO8M3pcfey+gFuBV4CdgN/Uep6FmgbX0/wMW0b8Ovsz60E488/BXYCPwEaS13rAv43uBn4fvbxhcAvgV3AQ0Cy1PUVeVtfA7Rn9/d3gIZK2NfAXwEvAM8B3wSSYdzXwP0E5xXGCD6p3TnX/gWM4IrA3cBvCK4mOqP31TdURURCqNyGZUREpAAKdxGREFK4i4iEkMJdRCSEFO4iIiGkcBcRCSGFu4hICCncRURC6P8DlxlswsD1JKIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(len(history_loss_train)), history_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move to utils.py\n",
    "def onehot_softmax(out):\n",
    "    \"\"\"convert output of softmax to onehot encoding\"\"\"\n",
    "    for \n",
    "    one_hot = np.zeros_like(out)\n",
    "    one_hot[np.argmax(out)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feed_forward(X_test, params)[\"A2\"]\n",
    "out_one_hot = np.apply_along_axis(onehot_softmax, axis=1, arr=out.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       980\n",
      "           1       0.97      0.98      0.98      1135\n",
      "           2       0.95      0.94      0.94      1032\n",
      "           3       0.93      0.95      0.94      1010\n",
      "           4       0.94      0.95      0.94       982\n",
      "           5       0.95      0.91      0.93       892\n",
      "           6       0.95      0.96      0.95       958\n",
      "           7       0.95      0.94      0.95      1028\n",
      "           8       0.94      0.93      0.93       974\n",
      "           9       0.93      0.93      0.93      1009\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      " samples avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test.T, out_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise:</b> Writing and training a perceptron with Momentum </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Towards datascience post](https://towardsdatascience.com/building-an-artificial-neural-network-using-pure-numpy-3fe21acc5815)\n",
    "- [Neural net from scratch](https://zhenye-na.github.io/2018/09/09/build-neural-network-with-mnist-from-scratch.html)\n",
    "- [How backpropagation works](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gantutorial",
   "language": "python",
   "name": "gantutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
