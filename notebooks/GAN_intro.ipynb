{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "## What is Adversarial?\n",
    "![indiana](https://media.giphy.com/media/tivaSkhu8MbKM/giphy.gif)\n",
    "\n",
    "$\\min_{\\text{boulder}}\\max_{\\text{indiana}} V(\\text{indiana}, \\text{boulder}) = \\text{distance between them}$\n",
    "\n",
    "\n",
    "Estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$ (cite Goodfellow2014)\n",
    "\n",
    "![overviewgan](../images/overview.png \"https://arxiv.org/pdf/1710.07035.pdf\")\n",
    "\n",
    "Image from Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta, B. and Bharath, A.A., 2018. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1), pp.53-65.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Notations\n",
    "\n",
    "- $x$ will be the **data element**.\n",
    "- $D(x)$ is the discriminator network which outputs the **probability that $x$ is real or generated**. $D(x)$ should be high when $x$ comes from training data and low when $x$ comes from the generator.\n",
    "- $z$ will be a **latent space vector** sampled from a normal distribution. $G(z)$ represents the generator function that which maps the latent vector $z$ to data-space. \n",
    "- The goal of $G$ is to estimate the distribution that the training data comes from ($p_{data}$) so it can generate fake samples from that estimated distribution ($p_g$).\n",
    "- $D(G(z))$ is the probability that the output of the generator $G$ is a real image. $D$ tries to **maximize the probability it correctly classifies reals and fakes** ($\\log D(x)$), and $G$ tries to **minimize the probability that $D$ will predict its outputs are fake** $(\\log (1 - D(G(x))$)\n",
    "\n",
    "$\\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(x)))\\big]$\n",
    "\n",
    "![overviewgan](../images/datadist.png \"https://arxiv.org/pdf/1710.07035.pdf\")\n",
    "\n",
    "During GAN training, the generator is encouraged to produce a distribution of samples, $p_g(x)$ to match that of real data $p_{data}(x)$.\n",
    "\n",
    "In theory, the solution to this minimax game is where $p_g=p_{data}$, and the discriminator guesses randomly if the inputs are real or fake. However, the convergence theory of GANs is still being actively researched and in reality models do not always train to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop of GANs training\n",
    "\n",
    "![overviewgan](../images/mainloop.png \"https://arxiv.org/pdf/1710.07035.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Develop Deep Learning Models with Pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of Layers\n",
    "**Conv Layers**\n",
    "\n",
    "In the convolution layers, the units are organized in feature maps, in which each unit is connected to local patches of the feature maps belonging to the previous layer through a set of weights, called filter bank. All units within a feature map share the same filter bank, different feature maps within the same layer use different filter banks, the arrangement of these has two justifications, on the one hand, data in form of arrays, as in our case, images, local subsets tend to be highly correlated and on the other hand the local statistics of images are invariant to their location.\n",
    "![conv](https://github.com/celiacintas/star_wars_hackathon/raw/8d46effee4e4a82429eb989f017a31c03a6bc2fd/images/Convolution_schematic.gif)\n",
    "\n",
    "Image from http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies a 2D convolution over an input signal composed of several input\n",
       "planes.\n",
       "\n",
       "In the simplest case, the output value of the layer with input size\n",
       ":math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
       "can be precisely described as:\n",
       "\n",
       ".. math::\n",
       "    \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
       "    \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
       "\n",
       "\n",
       "where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
       ":math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
       ":math:`H` is a height of input planes in pixels, and :math:`W` is\n",
       "width in pixels.\n",
       "\n",
       "* :attr:`stride` controls the stride for the cross-correlation, a single\n",
       "  number or a tuple.\n",
       "\n",
       "* :attr:`padding` controls the amount of implicit zero-paddings on both\n",
       "  sides for :attr:`padding` number of points for each dimension.\n",
       "\n",
       "* :attr:`dilation` controls the spacing between the kernel points; also\n",
       "  known as the Ã  trous algorithm. It is harder to describe, but this `link`_\n",
       "  has a nice visualization of what :attr:`dilation` does.\n",
       "\n",
       "* :attr:`groups` controls the connections between inputs and outputs.\n",
       "  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
       "  :attr:`groups`. For example,\n",
       "\n",
       "    * At groups=1, all inputs are convolved to all outputs.\n",
       "    * At groups=2, the operation becomes equivalent to having two conv\n",
       "      layers side by side, each seeing half the input channels,\n",
       "      and producing half the output channels, and both subsequently\n",
       "      concatenated.\n",
       "    * At groups= :attr:`in_channels`, each input channel is convolved with\n",
       "      its own set of filters, of size:\n",
       "      :math:`\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor`.\n",
       "\n",
       "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
       "\n",
       "    - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
       "    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
       "      and the second `int` for the width dimension\n",
       "\n",
       ".. note::\n",
       "\n",
       "     Depending of the size of your kernel, several (of the last)\n",
       "     columns of the input might be lost, because it is a valid `cross-correlation`_,\n",
       "     and not a full `cross-correlation`_.\n",
       "     It is up to the user to add proper padding.\n",
       "\n",
       ".. note::\n",
       "\n",
       "    When `groups == in_channels` and `out_channels == K * in_channels`,\n",
       "    where `K` is a positive integer, this operation is also termed in\n",
       "    literature as depthwise convolution.\n",
       "\n",
       "    In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`,\n",
       "    a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments\n",
       "    :math:`(in\\_channels=C_{in}, out\\_channels=C_{in} \\times K, ..., groups=C_{in})`.\n",
       "\n",
       ".. include:: cudnn_deterministic.rst\n",
       "\n",
       "Args:\n",
       "    in_channels (int): Number of channels in the input image\n",
       "    out_channels (int): Number of channels produced by the convolution\n",
       "    kernel_size (int or tuple): Size of the convolving kernel\n",
       "    stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
       "    padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
       "    padding_mode (string, optional). Accepted values `zeros` and `circular` Default: `zeros`\n",
       "    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
       "    groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
       "    bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C_{in}, H_{in}, W_{in})`\n",
       "    - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where\n",
       "\n",
       "      .. math::\n",
       "          H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
       "                    \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
       "\n",
       "      .. math::\n",
       "          W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
       "                    \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
       "\n",
       "Attributes:\n",
       "    weight (Tensor): the learnable weights of the module of shape\n",
       "                     :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
       "                     :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n",
       "                     The values of these weights are sampled from\n",
       "                     :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "                     :math:`k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
       "    bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,\n",
       "                     then the values of these weights are\n",
       "                     sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "                     :math:`k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # With square kernels and equal stride\n",
       "    >>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
       "    >>> # non-square kernels and unequal stride and with padding\n",
       "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
       "    >>> # non-square kernels and unequal stride and with padding and dilation\n",
       "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
       "    >>> input = torch.randn(20, 16, 50, 100)\n",
       "    >>> output = m(input)\n",
       "\n",
       ".. _cross-correlation:\n",
       "    https://en.wikipedia.org/wiki/Cross-correlation\n",
       "\n",
       ".. _link:\n",
       "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Code/.env/gantutorial/lib/python3.7/site-packages/torch/nn/modules/conv.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Conv2d, ConvBn2d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "nn.Conv2d?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(128, 64, 4, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MaxPooling**\n",
    "\n",
    "To reduce the dimensionality of feature maps, a pooling layer is located between the convolution layers. The pooling layers eliminate the non-maximum values by calculating an aggregation function, usually using maximum or average on small input regions. The main goal of these layers is to reduce the computational cost in subsequent layers by reducing the size of future feature maps and providing a form of translational invariance.\n",
    "![max](https://github.com/celiacintas/star_wars_hackathon/raw/8d46effee4e4a82429eb989f017a31c03a6bc2fd/images/maxpool.jpg)\n",
    "\n",
    "Image from http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BatchNorm \n",
    "- UpSampling/ConvTranspose2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization & Loss\n",
    "To take this search for the minimum error to practice we use **Stochastic Gradient Descent**, it consists of showing the input vectors of a subset of training data, compute the outputs, their errors, calculate the gradient for those examples and adjust the weights accordingly. This process is repeated over several subsets of examples until the objective function average stops decreasing.\n",
    "\n",
    "**explain ADAM**\n",
    "\n",
    "![alt text](https://github.com/celiacintas/star_wars_hackathon/raw/8d46effee4e4a82429eb989f017a31c03a6bc2fd/images/saddle_point_evaluation_optimizers.gif)\n",
    "\n",
    " Check out  http://sebastianruder.com/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " 'lr_scheduler']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "[x for x in dir(optim) if '__' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_optimizer = optim.Adam(conv.parameters(),\n",
    "                         lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different types of Loss implemented in PyTorch\n",
    "**Binary Cross Entropy**\n",
    "criterion that measures the Binary Cross Entropy between the target and the output, the loss can be defined as:\n",
    "\n",
    "$\\ell(x, y)=L=\\left\\{l_{1}, \\ldots, l_{N}\\right\\}^{\\top}, \\quad l_{n}=-w_{n}\\left[y_{n} \\cdot \\log x_{n}+\\left(1-y_{n}\\right) \\cdot \\log \\left(1-x_{n}\\right)\\right]$\n",
    "\n",
    "Where ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdaptiveLogSoftmaxWithLoss',\n",
       " 'BCELoss',\n",
       " 'BCEWithLogitsLoss',\n",
       " 'CTCLoss',\n",
       " 'CosineEmbeddingLoss',\n",
       " 'CrossEntropyLoss',\n",
       " 'HingeEmbeddingLoss',\n",
       " 'KLDivLoss',\n",
       " 'L1Loss',\n",
       " 'MSELoss',\n",
       " 'MarginRankingLoss',\n",
       " 'MultiLabelMarginLoss',\n",
       " 'MultiLabelSoftMarginLoss',\n",
       " 'MultiMarginLoss',\n",
       " 'NLLLoss',\n",
       " 'PoissonNLLLoss',\n",
       " 'SmoothL1Loss',\n",
       " 'SoftMarginLoss',\n",
       " 'TripletMarginLoss']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(nn) if x.endswith('Loss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as tfs\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_list = [tfs.RandomAffine(0., scale=(0.5, 1.), fillcolor=0),\n",
    "           tfs.Resize((16, 16)),\n",
    "           tfs.Grayscale(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_data = datasets.ImageFolder('data/XXXXX',\n",
    "                                     transform=tfs.Compose(ts_list))\n",
    "\n",
    "data_loader = data.DataLoader(imagenet_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize a few samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gantutorial",
   "language": "python",
   "name": "gantutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
